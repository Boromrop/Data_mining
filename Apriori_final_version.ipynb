{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Analysis with Apriori Algorithm\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive market basket analysis using the Apriori algorithm, featuring data preprocessing, association rule mining, customer segmentation, and interactive visualizations.\n",
    "\n",
    "## Key Features\n",
    "- **Data Loading & Cleaning**: Robust data preprocessing pipeline\n",
    "- **Exploratory Data Analysis**: Transaction pattern analysis\n",
    "- **Association Rule Mining**: Apriori algorithm implementation with mlxtend\n",
    "- **Customer Segmentation**: RFM analysis with K-means clustering\n",
    "- **Interactive Visualizations**: Plotly-based charts and network graphs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Setup\n",
    "\n",
    "Essential libraries for data analysis, visualization, and machine learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set display options for better output readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a logger function to track each step\n",
    "def log_step(step_name):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STEP: {step_name}\")\n",
    "    print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load and examine the transaction dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP: DATA LOADING\n",
      "================================================================================\n",
      "‚úó Error loading data: [Errno 2] No such file or directory: 'Assignment-1_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 1: DATA LOADING\n",
    "# =========================\n",
    "log_step(\"DATA LOADING\")\n",
    "\n",
    "def load_data(file_path, sep=';', dtype_dict=None):\n",
    "    \"\"\"\n",
    "    Load data from a file with specified parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the data file\n",
    "    sep : str, default ';'\n",
    "        Delimiter to use\n",
    "    dtype_dict : dict, default None\n",
    "        Dictionary of column data types\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Loaded DataFrame\n",
    "    \"\"\"\n",
    "    if dtype_dict is None:\n",
    "        dtype_dict = {'BillNo': str}\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=sep, dtype=dtype_dict)\n",
    "        print(f\"‚úì Successfully loaded data from {file_path}\")\n",
    "        print(f\"‚úì Dataset shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "purchase_df = load_data('Assignment-1_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Initial examination of dataset characteristics and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP: INITIAL DATA EXPLORATION\n",
      "================================================================================\n",
      "\n",
      "üìä Basic DataFrame Information:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Explore the loaded dataset\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mexplore_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpurchase_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mexplore_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Display basic information\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Basic DataFrame Information:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîç Missing Values Check:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 2: INITIAL DATA EXPLORATION\n",
    "# =========================\n",
    "log_step(\"INITIAL DATA EXPLORATION\")\n",
    "\n",
    "def explore_data(df):\n",
    "    \"\"\"\n",
    "    Perform initial exploration of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to explore\n",
    "    \"\"\"\n",
    "    # Display basic information\n",
    "    print(\"\\nüìä Basic DataFrame Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nüîç Missing Values Check:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found!\")\n",
    "    \n",
    "    # Show unique counts for categorical columns\n",
    "    print(\"\\nüî¢ Unique Value Counts:\")\n",
    "    print(df.nunique())\n",
    "    \n",
    "    # Display a sample of the data\n",
    "    print(\"\\nüìã Data Sample:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Generate descriptive statistics\n",
    "    print(\"\\nüìà Descriptive Statistics:\")\n",
    "    print(df.describe().round(2))\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(f\"\\nüîÑ Duplicate Rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# Explore the loaded dataset\n",
    "explore_data(purchase_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "Preprocessing pipeline to handle missing values, data types, and invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 3: DATA CLEANING\n",
    "# =========================\n",
    "log_step(\"DATA CLEANING\")\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values, converting data types,\n",
    "    and filtering out records with invalid values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to clean\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Track the original row count\n",
    "    original_rows = len(clean_df)\n",
    "    print(f\"Original row count: {original_rows}\")\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    try:\n",
    "        clean_df['Date'] = pd.to_datetime(clean_df['Date'], format='%d.%m.%Y %H:%M')\n",
    "        print(\"‚úì Converted 'Date' column to datetime\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error converting dates: {e}\")\n",
    "    \n",
    "    # Convert price column to float\n",
    "    try:\n",
    "        clean_df['Price'] = clean_df['Price'].str.replace(',', '.').astype(float)\n",
    "        print(\"‚úì Converted 'Price' column to float\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error converting prices: {e}\")\n",
    "    \n",
    "    # Convert CustomerID to string if it exists\n",
    "    if 'CustomerID' in clean_df.columns:\n",
    "        clean_df['CustomerID'] = clean_df['CustomerID'].astype('str')\n",
    "        print(\"‚úì Converted 'CustomerID' column to string\")\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    clean_df = clean_df.dropna()\n",
    "    print(f\"‚úì Dropped rows with missing values. Remaining: {len(clean_df)}\")\n",
    "    \n",
    "    # Filter out rows with non-positive quantity or price\n",
    "    clean_df = clean_df[clean_df['Quantity'] > 0]\n",
    "    clean_df = clean_df[clean_df['Price'] > 0]\n",
    "    print(f\"‚úì Filtered out rows with non-positive quantity or price. Remaining: {len(clean_df)}\")\n",
    "    \n",
    "    # Calculate and report the percentage of data retained\n",
    "    retained_pct = (len(clean_df) / original_rows) * 100\n",
    "    print(f\"‚úì Retained {retained_pct:.2f}% of original data after cleaning\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_purchase_df = clean_data(purchase_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Analyzing transaction patterns, customer behavior, and product popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 4: EXPLORATORY DATA ANALYSIS\n",
    "# =========================\n",
    "log_step(\"EXPLORATORY DATA ANALYSIS\")\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the cleaned dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Cleaned DataFrame to analyze\n",
    "    \"\"\"\n",
    "    # Temporal analysis\n",
    "    if 'Date' in df.columns:\n",
    "        # Extract date components\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['Day'] = df['Date'].dt.day\n",
    "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "        df['Hour'] = df['Date'].dt.hour\n",
    "        \n",
    "        print(\"\\nüìÖ Transaction distribution by month:\")\n",
    "        monthly_counts = df.groupby('Month').size()\n",
    "        print(monthly_counts)\n",
    "        \n",
    "        print(\"\\nüìÖ Transaction distribution by day of week:\")\n",
    "        day_counts = df.groupby('DayOfWeek').size()\n",
    "        print(day_counts.index.map({0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}))\n",
    "        print(day_counts.values)\n",
    "    \n",
    "    # Country analysis if available\n",
    "    if 'Country' in df.columns:\n",
    "        print(\"\\nüåç Transaction distribution by country:\")\n",
    "        country_counts = df['Country'].value_counts()\n",
    "        print(country_counts.head(10))\n",
    "    \n",
    "    # Product analysis\n",
    "    print(\"\\nüõí Top 10 most popular products:\")\n",
    "    top_products = df.groupby('Itemname').size().sort_values(ascending=False).head(10)\n",
    "    print(top_products)\n",
    "    \n",
    "    # Customer analysis\n",
    "    if 'CustomerID' in df.columns:\n",
    "        print(\"\\nüë§ Customer purchase frequency:\")\n",
    "        customer_purchases = df.groupby('CustomerID').size().describe().round(2)\n",
    "        print(customer_purchases)\n",
    "    \n",
    "    # Transaction value analysis\n",
    "    df['TransactionValue'] = df['Quantity'] * df['Price']\n",
    "    print(\"\\nüí∞ Transaction value statistics:\")\n",
    "    transaction_stats = df.groupby('BillNo')['TransactionValue'].sum().describe().round(2)\n",
    "    print(transaction_stats)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Perform EDA on the cleaned dataset\n",
    "df_with_eda = perform_eda(cleaned_purchase_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transaction Basket Creation\n",
    "\n",
    "Converting transaction data into market basket format for association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 5: BASKET TRANSFORMATION\n",
    "# =========================\n",
    "log_step(\"BASKET TRANSFORMATION\")\n",
    "\n",
    "def create_transaction_basket(df):\n",
    "    \"\"\"\n",
    "    Transform the transaction data into a format suitable for market basket analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Cleaned DataFrame with transaction data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Transaction data in basket format\n",
    "    pandas.DataFrame\n",
    "        One-hot encoded transaction data\n",
    "    \"\"\"\n",
    "    # Create transactions by grouping items by bill number\n",
    "    print(\"Creating transaction baskets...\")\n",
    "    transaction = df.groupby(['BillNo', 'Date'])['Itemname'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "    print(f\"‚úì Created {len(transaction)} transaction baskets\")\n",
    "    \n",
    "    # Display a sample of the transactions\n",
    "    print(\"\\nSample transactions:\")\n",
    "    print(transaction.head())\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    transaction.drop(columns=['BillNo', 'Date'], inplace=True)\n",
    "    \n",
    "    # Split the 'Itemname' column into separate rows\n",
    "    transaction = transaction.assign(Itemname=transaction['Itemname'].str.split(', ')).explode('Itemname')\n",
    "    \n",
    "    # Create a crosstab (binary matrix) of items per transaction\n",
    "    print(\"\\nCreating one-hot encoded transaction matrix...\")\n",
    "    basket_encoded = pd.crosstab(index=transaction.index, columns=transaction['Itemname'])\n",
    "    \n",
    "    # Convert to a binary matrix (0 or 1)\n",
    "    basket_encoded = basket_encoded.astype(bool).astype(int)\n",
    "    \n",
    "    print(f\"‚úì Created one-hot encoded matrix with {basket_encoded.shape[0]} transactions and {basket_encoded.shape[1]} unique items\")\n",
    "    \n",
    "    # Output sample of the binary matrix\n",
    "    print(\"\\nSample of one-hot encoded basket matrix:\")\n",
    "    print(basket_encoded.iloc[:5, :5])\n",
    "    \n",
    "    return transaction, basket_encoded\n",
    "\n",
    "# Create transaction basket and one-hot encoded matrix\n",
    "transaction_df, basket_matrix = create_transaction_basket(cleaned_purchase_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apriori Algorithm Implementation\n",
    "\n",
    "Mining frequent itemsets and generating association rules using the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 6: APRIORI ALGORITHM IMPLEMENTATION\n",
    "# =========================\n",
    "log_step(\"APRIORI ALGORITHM IMPLEMENTATION\")\n",
    "\n",
    "def apply_apriori(basket_matrix, min_support=0.01, min_confidence=0.5, min_lift=1.0):\n",
    "    \"\"\"\n",
    "    Apply the Apriori algorithm to generate frequent itemsets and association rules.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    basket_matrix : pandas.DataFrame\n",
    "        One-hot encoded transaction data\n",
    "    min_support : float, default 0.01\n",
    "        Minimum support threshold for frequent itemsets\n",
    "    min_confidence : float, default 0.5\n",
    "        Minimum confidence threshold for association rules\n",
    "    min_lift : float, default 1.0\n",
    "        Minimum lift threshold for association rules\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Frequent itemsets\n",
    "    pandas.DataFrame\n",
    "        Association rules\n",
    "    \"\"\"\n",
    "    # Generate frequent itemsets\n",
    "    print(f\"Generating frequent itemsets with minimum support = {min_support}...\")\n",
    "    try:\n",
    "        frequent_itemsets = apriori(basket_matrix, min_support=min_support, use_colnames=True)\n",
    "        print(f\"‚úì Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "        \n",
    "        # Display a sample of frequent itemsets\n",
    "        if not frequent_itemsets.empty:\n",
    "            print(\"\\nSample frequent itemsets:\")\n",
    "            print(frequent_itemsets.sort_values('support', ascending=False).head())\n",
    "        else:\n",
    "            print(\"No frequent itemsets found with the specified parameters.\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # Generate association rules\n",
    "        print(f\"\\nGenerating association rules with minimum confidence = {min_confidence}...\")\n",
    "        # Fix: The correct function call for association_rules in mlxtend\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        \n",
    "        # Filter rules by lift if specified\n",
    "        if min_lift > 1.0:\n",
    "            rules = rules[rules['lift'] >= min_lift]\n",
    "        \n",
    "        print(f\"‚úì Found {len(rules)} association rules\")\n",
    "        \n",
    "        # Display a sample of association rules\n",
    "        if not rules.empty:\n",
    "            # Convert frozensets to lists for better readability\n",
    "            sample_rules = rules.sort_values('lift', ascending=False).head().copy()\n",
    "            sample_rules['antecedents'] = sample_rules['antecedents'].apply(lambda x: list(x))\n",
    "            sample_rules['consequents'] = sample_rules['consequents'].apply(lambda x: list(x))\n",
    "            print(\"\\nSample association rules (sorted by lift):\")\n",
    "            print(sample_rules)\n",
    "        else:\n",
    "            print(\"No association rules found with the specified parameters.\")\n",
    "        \n",
    "        return frequent_itemsets, rules\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error applying Apriori algorithm: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Apply Apriori algorithm to the basket matrix\n",
    "frequent_itemsets, rules = apply_apriori(basket_matrix, min_support=0.01, min_confidence=0.5, min_lift=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Association Rules Visualization\n",
    "\n",
    "Interactive visualizations for exploring discovered patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 7: VISUALIZATION OF ASSOCIATION RULES\n",
    "# =========================\n",
    "log_step(\"VISUALIZATION OF ASSOCIATION RULES\")\n",
    "\n",
    "def visualize_association_rules(rules_df, itemsets_df=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for association rules analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rules_df : pandas.DataFrame\n",
    "        Association rules DataFrame\n",
    "    itemsets_df : pandas.DataFrame, optional\n",
    "        Frequent itemsets DataFrame\n",
    "    \"\"\"\n",
    "    if rules_df.empty:\n",
    "        print(\"‚ùå No association rules available for visualization\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Creating visualizations for {len(rules_df)} association rules...\")\n",
    "    \n",
    "    # 1. Scatter plot of Support vs Confidence\n",
    "    print(\"\\n1. Support vs Confidence Visualization\")\n",
    "    \n",
    "    # Create a copy of rules and convert frozensets to strings for visualization\n",
    "    plot_rules = rules_df.copy()\n",
    "    plot_rules['antecedents_str'] = plot_rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "    plot_rules['consequents_str'] = plot_rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "    \n",
    "    # Create a scatter plot using plotly\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add scatter trace\n",
    "    scatter = go.Scatter(\n",
    "        x=plot_rules['support'],\n",
    "        y=plot_rules['confidence'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=plot_rules['lift'] * 2,\n",
    "            color=plot_rules['lift'],\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title='Lift'),\n",
    "            showscale=True\n",
    "        ),\n",
    "        text=[\n",
    "            f\"Antecedents: {a}<br>\"\n",
    "            f\"Consequents: {c}<br>\"\n",
    "            f\"Support: {support:.3f}<br>\"\n",
    "            f\"Confidence: {confidence:.3f}<br>\"\n",
    "            f\"Lift: {lift:.3f}\"\n",
    "            for a, c, support, confidence, lift in zip(\n",
    "                plot_rules['antecedents_str'], \n",
    "                plot_rules['consequents_str'],\n",
    "                plot_rules['support'], \n",
    "                plot_rules['confidence'], \n",
    "                plot_rules['lift']\n",
    "            )\n",
    "        ],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(scatter)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Association Rules: Support vs Confidence',\n",
    "        xaxis_title='Support',\n",
    "        yaxis_title='Confidence',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created Support vs Confidence scatter plot\")\n",
    "    \n",
    "    # 2. Network Graph of Top Association Rules\n",
    "    print(\"\\n2. Network Graph Visualization\")\n",
    "    \n",
    "    # Select top rules by lift for the network graph (to avoid overcrowding)\n",
    "    top_rules = plot_rules.sort_values('lift', ascending=False).head(20)\n",
    "    \n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges with attributes\n",
    "    for _, row in top_rules.iterrows():\n",
    "        antecedent = row['antecedents_str']\n",
    "        consequent = row['consequents_str']\n",
    "        G.add_edge(\n",
    "            antecedent, \n",
    "            consequent, \n",
    "            weight=row['lift'],\n",
    "            support=row['support'],\n",
    "            confidence=row['confidence']\n",
    "        )\n",
    "    \n",
    "    # Get node positions\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    \n",
    "    # Create edge traces\n",
    "    edge_traces = []\n",
    "    for edge in G.edges(data=True):\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        \n",
    "        # Width based on lift\n",
    "        width = edge[2]['weight'] * 0.5\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=[x0, x1, None],\n",
    "            y=[y0, y1, None],\n",
    "            line=dict(width=width, color='rgba(150,150,150,0.7)'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines'\n",
    "        )\n",
    "        \n",
    "        edge_traces.append(edge_trace)\n",
    "    \n",
    "    # Create node trace\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_text.append(node)\n",
    "    \n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers+text',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color='skyblue',\n",
    "            line=dict(width=1, color='darkslategray')\n",
    "        ),\n",
    "        text=node_text,\n",
    "        textposition='top center',\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "    \n",
    "    # Create the figure\n",
    "    fig_network = go.Figure(\n",
    "        data=edge_traces + [node_trace],\n",
    "        layout=go.Layout(\n",
    "            title='Network of Top Association Rules by Lift',\n",
    "            showlegend=False,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20, l=5, r=5, t=40),\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created network graph of top association rules\")\n",
    "    \n",
    "    # 3. Heatmap of item co-occurrence (if itemsets are provided)\n",
    "    if itemsets_df is not None and not itemsets_df.empty:\n",
    "        print(\"\\n3. Item Co-occurrence Heatmap\")\n",
    "        \n",
    "        # Get top frequent items\n",
    "        top_items = []\n",
    "        for itemset in itemsets_df['itemsets']:\n",
    "            if len(itemset) == 1:\n",
    "                item = list(itemset)[0]\n",
    "                top_items.append((item, itemsets_df.loc[itemsets_df['itemsets'] == itemset, 'support'].values[0]))\n",
    "        \n",
    "        # Sort by support and take top 15\n",
    "        top_items = sorted(top_items, key=lambda x: x[1], reverse=True)[:15]\n",
    "        top_item_names = [item[0] for item in top_items]\n",
    "        \n",
    "        # Create co-occurrence matrix\n",
    "        cooc_matrix = np.zeros((len(top_item_names), len(top_item_names)))\n",
    "        \n",
    "        # Fill co-occurrence matrix\n",
    "        for i, item1 in enumerate(top_item_names):\n",
    "            for j, item2 in enumerate(top_item_names):\n",
    "                if i == j:\n",
    "                    # Diagonal - use the item's own support\n",
    "                    cooc_matrix[i, j] = next(item[1] for item in top_items if item[0] == item1)\n",
    "                else:\n",
    "                    # Off-diagonal - find rules containing both items\n",
    "                    pair_rules = rules_df[\n",
    "                        rules_df['antecedents'].apply(lambda x: item1 in x and item2 in x) |\n",
    "                        rules_df['consequents'].apply(lambda x: item1 in x and item2 in x) |\n",
    "                        (rules_df['antecedents'].apply(lambda x: item1 in x) & \n",
    "                         rules_df['consequents'].apply(lambda x: item2 in x)) |\n",
    "                        (rules_df['antecedents'].apply(lambda x: item2 in x) & \n",
    "                         rules_df['consequents'].apply(lambda x: item1 in x))\n",
    "                    ]\n",
    "                    \n",
    "                    if not pair_rules.empty:\n",
    "                        cooc_matrix[i, j] = pair_rules['support'].max()\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "            z=cooc_matrix,\n",
    "            x=top_item_names,\n",
    "            y=top_item_names,\n",
    "            colorscale='Viridis',\n",
    "            hoverongaps=False\n",
    "        ))\n",
    "        \n",
    "        fig_heatmap.update_layout(\n",
    "            title='Item Co-occurrence Heatmap (based on support)',\n",
    "            xaxis=dict(tickangle=-45),\n",
    "            yaxis=dict(tickangle=0)\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì Created item co-occurrence heatmap\")\n",
    "    \n",
    "    print(\"\\n‚úì All visualizations created successfully\")\n",
    "\n",
    "# Visualize the generated association rules\n",
    "if 'rules' in locals() and not rules.empty:\n",
    "    visualize_association_rules(rules, frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Country-Specific Analysis\n",
    "\n",
    "Regional market basket analysis to understand geographical patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 8: COUNTRY-SPECIFIC ANALYSIS\n",
    "# =========================\n",
    "log_step(\"COUNTRY-SPECIFIC ANALYSIS\")\n",
    "\n",
    "def analyze_by_country(df, country_name):\n",
    "    \"\"\"\n",
    "    Perform country-specific market basket analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Cleaned DataFrame with transaction data\n",
    "    country_name : str\n",
    "        Name of the country to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Country-specific frequent itemsets\n",
    "    pandas.DataFrame\n",
    "        Country-specific association rules\n",
    "    \"\"\"\n",
    "    # Check if Country column exists\n",
    "    if 'Country' not in df.columns:\n",
    "        print(\"‚ùå Country column not available in dataset\")\n",
    "        return None, None\n",
    "    \n",
    "    # Filter by country\n",
    "    country_df = df[df['Country'] == country_name]\n",
    "    \n",
    "    if len(country_df) == 0:\n",
    "        print(f\"‚ùå No transactions found for country: {country_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Analyzing transactions from {country_name}...\")\n",
    "    print(f\"‚úì Found {len(country_df)} transactions from {country_name}\")\n",
    "    \n",
    "    # Create transaction basket for the country\n",
    "    country_transaction, country_basket = create_transaction_basket(country_df)\n",
    "    \n",
    "    # Apply Apriori algorithm to the country-specific basket\n",
    "    country_itemsets, country_rules = apply_apriori(country_basket, min_support=0.01, min_confidence=0.5)\n",
    "    \n",
    "    # Visualize country-specific rules\n",
    "    if not country_rules.empty:\n",
    "        print(f\"\\nVisualizing association rules for {country_name}...\")\n",
    "        visualize_association_rules(country_rules, country_itemsets)\n",
    "    \n",
    "    return country_itemsets, country_rules\n",
    "\n",
    "# Perform country-specific analysis (e.g., for United Kingdom)\n",
    "if 'Country' in cleaned_purchase_df.columns:\n",
    "    uk_itemsets, uk_rules = analyze_by_country(cleaned_purchase_df, 'United Kingdom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Customer Segmentation with RFM Analysis\n",
    "\n",
    "Understanding customer behavior through Recency, Frequency, and Monetary analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 9: RFM ANALYSIS\n",
    "# =========================\n",
    "log_step(\"RFM ANALYSIS\")\n",
    "\n",
    "def perform_rfm_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform RFM (Recency, Frequency, Monetary) analysis on the transaction data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Cleaned DataFrame with transaction data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        RFM scores by customer\n",
    "    \"\"\"\n",
    "    # Check if necessary columns exist\n",
    "    required_cols = ['CustomerID', 'Date', 'BillNo']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"‚ùå {col} column not available for RFM analysis\")\n",
    "            return None\n",
    "    \n",
    "    print(\"Computing RFM metrics...\")\n",
    "    \n",
    "    # Add monetary value if not already present\n",
    "    if 'TransactionValue' not in df.columns:\n",
    "        df['TransactionValue'] = df['Quantity'] * df['Price']\n",
    "    \n",
    "    # Define today's date for recency calculation (use the most recent date in the dataset)\n",
    "    today_date = df['Date'].max()\n",
    "    print(f\"Reference date for recency calculation: {today_date}\")\n",
    "    \n",
    "    # Group by CustomerID to calculate RFM metrics\n",
    "    rfm = df.groupby('CustomerID').agg({\n",
    "        'Date': lambda x: (today_date - x.max()).days,   # Recency\n",
    "        'BillNo': 'nunique',                             # Frequency\n",
    "        'TransactionValue': 'sum'                         # Monetary\n",
    "    })\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    rfm.columns = ['recency', 'frequency', 'monetary']\n",
    "    \n",
    "    # Reset index\n",
    "    rfm = rfm.reset_index()\n",
    "    \n",
    "    print(\"‚úì RFM metrics calculated successfully\")\n",
    "    print(f\"‚úì Number of unique customers analyzed: {len(rfm)}\")\n",
    "    \n",
    "    # Display summary statistics of RFM metrics\n",
    "    print(\"\\nRFM metrics summary:\")\n",
    "    print(rfm[['recency', 'frequency', 'monetary']].describe().round(2))\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "# Perform RFM analysis\n",
    "rfm_data = perform_rfm_analysis(cleaned_purchase_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. K-Means Clustering on RFM Data\n",
    "\n",
    "Advanced customer segmentation using machine learning clustering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 10: CUSTOMER SEGMENTATION\n",
    "# =========================\n",
    "log_step(\"CUSTOMER SEGMENTATION\")\n",
    "\n",
    "def perform_customer_segmentation(rfm_data, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Perform customer segmentation using K-means clustering on RFM metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rfm_data : pandas.DataFrame\n",
    "        RFM metrics by customer\n",
    "    n_clusters : int, default 3\n",
    "        Number of clusters to create\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        RFM data with cluster assignments\n",
    "    pandas.DataFrame\n",
    "        Cluster profiles\n",
    "    \"\"\"\n",
    "    if rfm_data is None:\n",
    "        print(\"‚ùå No RFM data available for segmentation\")\n",
    "        return None, None\n",
    "    \n",
    "    # Handle extreme outliers before scaling (optional but recommended)\n",
    "    rfm_clean = rfm_data.copy()\n",
    "    for col in ['recency', 'frequency', 'monetary']:\n",
    "        q1 = rfm_clean[col].quantile(0.01)  # 1st percentile\n",
    "        q3 = rfm_clean[col].quantile(0.99)  # 99th percentile\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (1.5 * iqr)\n",
    "        upper_bound = q3 + (1.5 * iqr)\n",
    "        \n",
    "        # Cap extreme outliers\n",
    "        rfm_clean.loc[rfm_clean[col] > upper_bound, col] = upper_bound\n",
    "        rfm_clean.loc[rfm_clean[col] < lower_bound, col] = lower_bound\n",
    "    \n",
    "    # Normalize the RFM metrics\n",
    "    print(\"Normalizing RFM metrics...\")\n",
    "    scaler = StandardScaler()\n",
    "    rfm_scaled = scaler.fit_transform(rfm_clean[['recency', 'frequency', 'monetary']])\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    rfm_clean['cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "    \n",
    "    # Transfer cluster assignments back to original data\n",
    "    rfm_data['cluster'] = rfm_clean['cluster']\n",
    "    \n",
    "    print(\"‚úì Clustering completed successfully\")\n",
    "    \n",
    "    # Generate cluster profiles\n",
    "    cluster_profile = rfm_data.groupby('cluster').agg({\n",
    "        'recency': 'mean',\n",
    "        'frequency': 'mean',\n",
    "        'monetary': 'mean',\n",
    "        'CustomerID': 'count'  # Count of customers in each cluster\n",
    "    }).rename(columns={'CustomerID': 'n_customers'})\n",
    "    \n",
    "    # Display cluster profiles\n",
    "    print(\"\\nCluster profiles:\")\n",
    "    print(cluster_profile.round(2))\n",
    "    \n",
    "    # Interpret clusters\n",
    "    print(\"\\nCluster Interpretation:\")\n",
    "    for cluster in range(n_clusters):\n",
    "        profile = cluster_profile.loc[cluster]\n",
    "        print(f\"\\nCluster {cluster} ({profile['n_customers']} customers):\")\n",
    "        \n",
    "        # Recency interpretation (lower is better)\n",
    "        if profile['recency'] <= cluster_profile['recency'].quantile(0.33):\n",
    "            recency_desc = \"Recent customers\"\n",
    "        elif profile['recency'] <= cluster_profile['recency'].quantile(0.67):\n",
    "            recency_desc = \"Moderately recent customers\"\n",
    "        else:\n",
    "            recency_desc = \"Less recent customers\"\n",
    "        \n",
    "        # Frequency interpretation (higher is better)\n",
    "        if profile['frequency'] >= cluster_profile['frequency'].quantile(0.67):\n",
    "            frequency_desc = \"Frequent shoppers\"\n",
    "        elif profile['frequency'] >= cluster_profile['frequency'].quantile(0.33):\n",
    "            frequency_desc = \"Moderate shoppers\"\n",
    "        else:\n",
    "            frequency_desc = \"Infrequent shoppers\"\n",
    "        \n",
    "        # Monetary interpretation (higher is better)\n",
    "        if profile['monetary'] >= cluster_profile['monetary'].quantile(0.67):\n",
    "            monetary_desc = \"High spenders\"\n",
    "        elif profile['monetary'] >= cluster_profile['monetary'].quantile(0.33):\n",
    "            monetary_desc = \"Medium spenders\"\n",
    "        else:\n",
    "            monetary_desc = \"Low spenders\"\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {recency_desc}, {frequency_desc}, {monetary_desc}\")\n",
    "        print(f\"  ‚Ä¢ Avg. Days Since Last Purchase: {profile['recency']:.1f}\")\n",
    "        print(f\"  ‚Ä¢ Avg. Purchase Frequency: {profile['frequency']:.1f}\")\n",
    "        print(f\"  ‚Ä¢ Avg. Monetary Value: ${profile['monetary']:.2f}\")\n",
    "    \n",
    "    # Create visualizations of the clusters\n",
    "    print(\"\\nCreating customer segmentation visualizations...\")\n",
    "    \n",
    "    # 1. 3D Scatter Plot of RFM Clusters\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add a scatter trace for each cluster\n",
    "    for cluster_id in sorted(rfm_data['cluster'].unique()):\n",
    "        cluster_data = rfm_data[rfm_data['cluster'] == cluster_id]\n",
    "        \n",
    "        # Choose cluster colors\n",
    "        cluster_colors = {0: 'rgb(31, 119, 180)', 1: 'rgb(255, 127, 14)', 2: 'rgb(44, 160, 44)', \n",
    "                          3: 'rgb(214, 39, 40)', 4: 'rgb(148, 103, 189)'}\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=cluster_data['recency'],\n",
    "            y=cluster_data['frequency'],\n",
    "            z=cluster_data['monetary'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=cluster_colors.get(cluster_id, f'rgb({(cluster_id*50) % 255}, {(cluster_id*80) % 255}, {(cluster_id*120) % 255})'),\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            name=f'Cluster {cluster_id}'\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='3D Customer Segmentation based on RFM',\n",
    "        scene=dict(\n",
    "            xaxis_title='Recency (days)',\n",
    "            yaxis_title='Frequency (purchases)',\n",
    "            zaxis_title='Monetary (value)'\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "            traceorder=\"normal\",\n",
    "            font=dict(\n",
    "                family=\"sans-serif\",\n",
    "                size=12,\n",
    "                color=\"black\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created 3D visualization of customer segments\")\n",
    "    \n",
    "    # 2. RFM Parallel Coordinates Plot\n",
    "    # Normalize the data for better visualization\n",
    "    rfm_viz = rfm_data.copy()\n",
    "    \n",
    "    # Invert recency so that higher is better (like frequency and monetary)\n",
    "    rfm_viz['recency_inv'] = rfm_viz['recency'].max() - rfm_viz['recency']\n",
    "    \n",
    "    # Create parallel coordinates plot\n",
    "    fig_parallel = go.Figure(data=\n",
    "        go.Parcoords(\n",
    "            line=dict(\n",
    "                color=rfm_viz['cluster'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True\n",
    "            ),\n",
    "            dimensions=list([\n",
    "                dict(range=[rfm_viz['recency_inv'].min(), rfm_viz['recency_inv'].max()],\n",
    "                     label='Recency (Inverted)', \n",
    "                     values=rfm_viz['recency_inv']),\n",
    "                dict(range=[rfm_viz['frequency'].min(), rfm_viz['frequency'].max()],\n",
    "                     label='Frequency', \n",
    "                     values=rfm_viz['frequency']),\n",
    "                dict(range=[rfm_viz['monetary'].min(), rfm_viz['monetary'].max()],\n",
    "                     label='Monetary', \n",
    "                     values=rfm_viz['monetary'])\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig_parallel.update_layout(\n",
    "        title='Parallel Coordinates Plot of RFM Segments'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created parallel coordinates plot of RFM metrics\")\n",
    "    \n",
    "    # 3. Radar Chart for Cluster Profiles\n",
    "    # Prepare data for radar chart\n",
    "    radar_data = cluster_profile[['recency', 'frequency', 'monetary']].copy()\n",
    "    \n",
    "    # Invert recency so that higher values are better (closer to center of radar)\n",
    "    radar_data['recency'] = radar_data['recency'].max() - radar_data['recency']\n",
    "    \n",
    "    # Normalize each metric to 0-1 scale for radar chart\n",
    "    for col in radar_data.columns:\n",
    "        if radar_data[col].max() > 0:\n",
    "            radar_data[col] = radar_data[col] / radar_data[col].max()\n",
    "    \n",
    "    # Create radar chart\n",
    "    categories = ['Recency', 'Frequency', 'Monetary']\n",
    "    fig_radar = go.Figure()\n",
    "    \n",
    "    for cluster_id in sorted(radar_data.index):\n",
    "        fig_radar.add_trace(go.Scatterpolar(\n",
    "            r=radar_data.loc[cluster_id].values.tolist(),\n",
    "            theta=categories,\n",
    "            fill='toself',\n",
    "            name=f'Cluster {cluster_id}'\n",
    "        ))\n",
    "    \n",
    "    fig_radar.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        title='Radar Chart of Cluster Profiles'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Created radar chart of cluster profiles\")\n",
    "    \n",
    "    return rfm_data, cluster_profile\n",
    "\n",
    "# Perform customer segmentation using K-means clustering\n",
    "if rfm_data is not None:\n",
    "    segmented_rfm, cluster_profiles = perform_customer_segmentation(rfm_data, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Business Recommendations\n",
    "\n",
    "Key insights and actionable recommendations from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 11: SUMMARY AND RECOMMENDATIONS\n",
    "# =========================\n",
    "log_step(\"SUMMARY AND RECOMMENDATIONS\")\n",
    "\n",
    "def summarize_analysis():\n",
    "    \"\"\"\n",
    "    Summarize the market basket analysis and provide recommendations.\n",
    "    \"\"\"\n",
    "    print(\"üìä MARKET BASKET ANALYSIS SUMMARY üìä\")\n",
    "    print(\"\\n1. Data Quality Assessment:\")\n",
    "    print(\"   ‚Ä¢ Initial dataset size and quality checked\")\n",
    "    print(\"   ‚Ä¢ Missing values and outliers handled\")\n",
    "    print(\"   ‚Ä¢ Data type conversions performed\")\n",
    "    \n",
    "    print(\"\\n2. Transaction Analysis:\")\n",
    "    print(\"   ‚Ä¢ Transaction patterns explored across time and geography\")\n",
    "    print(\"   ‚Ä¢ Popular products and purchasing patterns identified\")\n",
    "    \n",
    "    print(\"\\n3. Association Rules:\")\n",
    "    print(\"   ‚Ä¢ Frequent itemsets generated using Apriori algorithm\")\n",
    "    print(\"   ‚Ä¢ Association rules discovered based on support and confidence thresholds\")\n",
    "    \n",
    "    print(\"\\n4. Customer Segmentation:\")\n",
    "    print(\"   ‚Ä¢ RFM analysis performed to understand customer behavior\")\n",
    "    print(\"   ‚Ä¢ Customer segments identified via clustering\")\n",
    "    \n",
    "    print(\"\\nüöÄ ACTIONABLE RECOMMENDATIONS üöÄ\")\n",
    "    print(\"\\n1. Product Placement and Bundling:\")\n",
    "    print(\"   ‚Ä¢ Arrange frequently co-purchased items together on shelves\")\n",
    "    print(\"   ‚Ä¢ Create product bundles based on high-confidence association rules\")\n",
    "    \n",
    "    print(\"\\n2. Targeted Marketing:\")\n",
    "    print(\"   ‚Ä¢ Design personalized promotions for each customer segment\")\n",
    "    print(\"   ‚Ä¢ Cross-sell products based on association rules\")\n",
    "    \n",
    "    print(\"\\n3. Inventory Management:\")\n",
    "    print(\"   ‚Ä¢ Optimize inventory based on frequent itemsets\")\n",
    "    print(\"   ‚Ä¢ Ensure complementary products are always in stock together\")\n",
    "    \n",
    "    print(\"\\n4. Website/Store Layout:\")\n",
    "    print(\"   ‚Ä¢ Optimize navigation to place associated products near each other\")\n",
    "    print(\"   ‚Ä¢ Implement 'Frequently Bought Together' recommendations\")\n",
    "    \n",
    "    print(\"\\n5. Next Steps:\")\n",
    "    print(\"   ‚Ä¢ Continuously update the analysis with new transaction data\")\n",
    "    print(\"   ‚Ä¢ Conduct A/B testing to validate recommendations\")\n",
    "    print(\"   ‚Ä¢ Integrate findings with other business metrics\")\n",
    "\n",
    "# Generate summary and recommendations\n",
    "summarize_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
